# -*- coding: utf-8 -*-
import os
import time
import requests
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util.ssl_ import create_urllib3_context
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote, quote
import re
import urllib3
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ================== é…ç½® ==================
BASE_URL = "https://learn.lianglianglee.com/"
OUTPUT_DIR = "æŠ€æœ¯æ‘˜æŠ„"
BOOK_LIST_FILE = "doc.txt"
MAX_WORKERS = 1  # æ”¹ä¸ºå•çº¿ç¨‹

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Safari/605.1.15',
]


# =========================================

class CustomHTTPAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False, **pool_kwargs):
        ctx = create_urllib3_context()
        ctx.set_ciphers('DEFAULT@SECLEVEL=1')
        self.poolmanager = PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_context=ctx,
            assert_hostname=False,
            cert_reqs='CERT_NONE',
            **pool_kwargs
        )

def clean_filename(name):
    name = unquote(name)
    name = re.sub(r'[<>:"/\\|?*\x00-\x1f]', '_', name.strip())
    return re.sub(r'\s+', '_', name)[:100] or "unnamed"

def load_books():
    if not os.path.exists(BOOK_LIST_FILE):
        raise FileNotFoundError(f"âŒ è¯·åˆ›å»º {BOOK_LIST_FILE} æ–‡ä»¶ï¼ˆUTF-8 ç¼–ç ï¼‰ï¼Œæ¯è¡Œä¸€ä¸ªä¸“æ å")
    with open(BOOK_LIST_FILE, 'r', encoding='utf-8') as f:
        books = [line.strip() for line in f if line.strip()]
    return books

def download_global_static(output_dir):
    """ä¸‹è½½å…¨å±€é™æ€èµ„æºåˆ° static/ ç›®å½•"""
    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=10,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = CustomHTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.headers.update({'User-Agent': random.choice(USER_AGENTS)})

    static_dir = os.path.join(output_dir, "static")
    os.makedirs(static_dir, exist_ok=True)
    static_files = [
        "index.css", "highlight.min.css", "highlight.min.js",
        "index.js", "main.js", "email-decode.min.js", "favicon.png"
    ]
    for fname in static_files:
        url = urljoin(BASE_URL, f"/static/{fname}")
        local_path = os.path.join(static_dir, fname)
        if os.path.exists(local_path):
            continue
        try:
            resp = session.get(url, timeout=15)
            if resp.status_code == 200:
                with open(local_path, 'wb') as f:
                    f.write(resp.content)
                print(f"  ğŸ“¦ ä¸‹è½½é™æ€èµ„æº: {fname}")
        except Exception as e:
            print(f"  âš ï¸ é™æ€èµ„æºå¤±è´¥ {fname}: {e}")
        time.sleep(0.1)

def fix_links_in_html(html_content, md_to_html_map, book_dir):
    """ä¿®å¤ HTML ä¸­çš„é“¾æ¥ï¼š.md â†’ .htmlï¼Œ/static/ â†’ ../static/"""
    soup = BeautifulSoup(html_content, 'html.parser')

    # 1. ä¿®å¤ /static/ è·¯å¾„
    for tag in soup.find_all(['link', 'script']):
        if tag.get('href', '').startswith('/static/'):
            tag['href'] = '../static/' + tag['href'].split('/static/')[-1]
        if tag.get('src', '').startswith('/static/'):
            tag['src'] = '../static/' + tag['src'].split('/static/')[-1]

    # 2. ä¿®å¤ faviconï¼ˆé¦–é¡µé“¾æ¥ï¼‰
    for img in soup.select('img[src="/static/favicon.png"]'):
        img['src'] = '../static/favicon.png'

    # 3. ä¿®å¤å†…éƒ¨ .md é“¾æ¥ â†’ .html
    for a in soup.find_all('a', href=True):
        href = a['href']
        if href.endswith('.md'):
            # å°è¯•åŒ¹é…æ˜ å°„
            filename_md = href.split('/')[-1]
            target_html = filename_md.replace('.md', '.html')
            if target_html in md_to_html_map.values():
                a['href'] = target_html
        elif href == '/':
            a['href'] = '../../index.html'
        elif href == '../':
            a['href'] = '../index.html'

    return str(soup)

def download_single_page(args):
    full_url, md_filename, html_filename, book_dir, asset_dir, md_to_html_map = args
    local_path = os.path.join(book_dir, html_filename)
    if os.path.exists(local_path):
        return f"âœ… å·²å­˜åœ¨: {html_filename}"

    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=10,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = CustomHTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.headers.update({'User-Agent': random.choice(USER_AGENTS)})

    # éšæœºå»¶è¿Ÿ
    delay = random.uniform(8, 15)
    time.sleep(delay)
    try:
        resp = session.get(full_url, timeout=15)
        resp.raise_for_status()
        content = resp.text

        # ä¸‹è½½å›¾ç‰‡
        page_soup = BeautifulSoup(content, 'html.parser')
        for img in page_soup.find_all('img', src=True):
            img_src = img['src']
            if img_src.startswith('assets/'):
                abs_img_url = urljoin(full_url, img_src)
                try:
                    img_resp = session.get(abs_img_url, timeout=15)
                    if img_resp.status_code == 200:
                        os.makedirs(asset_dir, exist_ok=True)
                        img_name = unquote(img_src.split('/')[-1])
                        img_local = os.path.join(asset_dir, clean_filename(img_name))
                        with open(img_local, 'wb') as f:
                            f.write(img_resp.content)
                except:
                    pass

        # ä¿®å¤é“¾æ¥
        fixed_html = fix_links_in_html(content, md_to_html_map, book_dir)

        with open(local_path, 'w', encoding='utf-8') as f:
            f.write(fixed_html)
        return f"âœ… å·²ä¿å­˜: {html_filename}"
    except Exception as e:
        return f"âŒ å¤±è´¥ {html_filename}: {str(e)[:60]}"

def main():
    books = load_books()
    print(f"ğŸ“š ä» {BOOK_LIST_FILE} åŠ è½½ {len(books)} ä¸ªä¸“æ ")

    # ä¸‹è½½å…¨å±€é™æ€èµ„æº
    print("ğŸ“¥ ä¸‹è½½å…¨å±€é™æ€èµ„æº...")
    download_global_static(OUTPUT_DIR)

    for book in books:
        book_clean = clean_filename(book)
        book_dir = os.path.join(OUTPUT_DIR, book_clean)
        asset_dir = os.path.join(book_dir, "assets")
        os.makedirs(book_dir, exist_ok=True)
        print(f"\nğŸ“¥ å¤„ç†ä¸“æ : {book_clean}")

        safe_book = quote(book, safe='')
        book_url = f"{BASE_URL}%E4%B8%93%E6%A0%8F/{safe_book}/"

        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=10,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = CustomHTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        session.headers.update({'User-Agent': random.choice(USER_AGENTS)})

        try:
            resp = session.get(book_url, timeout=15)
            resp.raise_for_status()
        except Exception as e:
            print(f"  âš ï¸ ä¸“æ é¡µé¢å¤±è´¥: {e}")
            time.sleep(3)
            continue

        soup = BeautifulSoup(resp.text, 'html.parser')
        md_links = []
        for a in soup.select('a[href$=".md"]'):
            href = a['href']
            full_url = urljoin(book_url, href)
            md_filename = href.split('/')[-1]
            html_filename = clean_filename(md_filename.replace('.md', '.html'))
            md_links.append((full_url, md_filename, html_filename))

        if not md_links:
            print("  âš ï¸ æœªæ‰¾åˆ°ä»»ä½• .md é¡µé¢")
            continue

        # æ„å»ºæ˜ å°„ï¼šç”¨äºé“¾æ¥ä¿®å¤
        md_to_html_map = {md: html for _, md, html in md_links}

        print(f"  ğŸ“„ æ‰¾åˆ° {len(md_links)} ä¸ªé¡µé¢ï¼Œå¼€å§‹å¹¶å‘ä¸‹è½½ï¼ˆ{MAX_WORKERS}çº¿ç¨‹ï¼‰...")

        # å‡†å¤‡å‚æ•°
        tasks = [
            (full_url, md_filename, html_filename, book_dir, asset_dir, md_to_html_map)
            for full_url, md_filename, html_filename in md_links
        ]

        # å¹¶å‘ä¸‹è½½
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(download_single_page, task) for task in tasks]
            for future in as_completed(futures):
                print(f"  {future.result()}")
                time.sleep(5)  # é˜²æ­¢å•çº¿ç¨‹è¿‡å¿«

        time.sleep(2)

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼ç¦»çº¿å†…å®¹ä¿å­˜è‡³ ./{OUTPUT_DIR}/")

if __name__ == "__main__":
    main()